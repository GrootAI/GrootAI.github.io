---
date: 2020-11-16T01:50:00.000Z
layout: post
title: Why bigger is not always better&#58; on finite and infinite neural networks
subtitle: The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance.
description: The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance.
optimized_image: https://res.cloudinary.com/dfnkx5mr1/image/upload/c_limit,h_200,w_380/v1609603777/post_img/is-bigger-better_wvltyv.jpg
category: Deep Learning
tags:
  - Deep Learning Theory
  - Bayesian Statistics
author: Kyuyong-Shin
---

### Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/6wuke6XWMf0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### References
[Why bigger is not always better&#58; on finite and infinite neural networks](https://proceedings.icml.cc/static/paper_files/icml/2020/3680-Paper.pdf)
