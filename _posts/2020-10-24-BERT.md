---
date: 2020-10-24T18:00:00.000Z
layout: post
title: BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding
subtitle: Get the ultimate pre-trained features applicable to any language model
description: Get the ultimate pre-trained features applicable to any language model
image: https://res.cloudinary.com/dthouk4zq/image/upload/v1603528258/transfer_nzqjnh.png
optimized_image: https://res.cloudinary.com/dthouk4zq/image/upload/c_scale,w_380/v1603528258/transfer_nzqjnh.png
category: NLP
tags:
  - NLP
author: Kyuyong-Shin
---

### Video 
<iframe width="560" height="315" src="https://www.youtube.com/embed/3HDYcSZae54" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.

BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations.