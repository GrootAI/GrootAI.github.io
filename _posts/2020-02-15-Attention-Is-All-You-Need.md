---
date: 2020-02-15 22:55:45
layout: post
title: Attention Is All You Need
subtitle: What’s the structure in my dataset or what are the symmetries in my dataset and is there a model that exists that has the inductive biases to model these properties that exist in my dataset.
description: What’s the structure in my dataset or what are the symmetries in my dataset and is there a model that exists that has the inductive biases to model these properties that exist in my dataset.
image: https://res.cloudinary.com/dfnkx5mr1/image/upload/v1602307993/post_img/luong2015-fig2-3_fpyymg.png
optimized_image: https://res.cloudinary.com/dfnkx5mr1/image/upload/v1602307993/post_img/luong2015-fig2-3_fpyymg.png
category: NLP
tags:
  - NLP
author: Kyuyong-Shin
---

### Video 
<iframe width="560" height="315" src="https://www.youtube.com/embed/gGDOc8vWmz0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Slide
<iframe src="//www.slideshare.net/slideshow/embed_code/key/b10XhimTLNXdTY" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:0px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> 

Deep learning is all about representation learning, building the right tools for learning representations is important factor in 
achieving empirical success.

They propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 
Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
