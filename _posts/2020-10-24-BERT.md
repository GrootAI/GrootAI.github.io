---
date: 2020-10-24T15:00:00.000Z
layout: post
title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
subtitle: Get the ultimate pre-trained features applicable to any language model
description: Get the ultimate pre-trained features applicable to any language model
image: >-
  https://res.cloudinary.com/dthouk4zq/image/upload/v1603525851/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f612f61642f4b6e6f776c656467655f7472616e736665722e7376672f33313270782d4b6e6f776c656467655f7472616e736665722e7376_smtg4h.png
optimized_image: >-
  https://res.cloudinary.com/dthouk4zq/image/upload/c_scale,w_380/v1603525851/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f612f61642f4b6e6f776c656467655f7472616e736665722e7376672f33313270782d4b6e6f776c656467655f7472616e736665722e7376_smtg4h.png
category: Natural Language Processing
tags:
  - NLP
author: Kyuyong-Shin
---

### Video 
<iframe width="560" height="315" src="https://www.youtube.com/embed/3HDYcSZae54" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.

BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations.